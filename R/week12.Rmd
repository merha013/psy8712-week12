---
title: "week12"
author: "Merhar"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Script Settings and Resources

```{r}
library(tidyverse)
library(RedditExtractoR)
library(jsonlite)
library(tm)
library(qdap)
library(textstem)
library(RWeka)
library(wordcloud)
library(tidytext)
library(ldatuning)
library(topicmodels)
library(randomForest)
library(haven)
library(caret)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
```

## Data Import and Cleaning

```{r}
# data <- find_thread_urls( 
#  subreddit = "IOPsychology", 
#  sort_by = "new",
#  period = "year") # pulling 1 years worth of data
# write_csv(data, file = "../data/week12data.csv")
# urls <- data$url # defining url values to put in next line
# thread_details <- get_thread_content(urls) # expect a LONG processing time
  # thread_details gives access to upvotes, which wasn't provided in 'data'.

# week12_tbl <- tibble( # create the tibble with upvotes & title
#  title = thread_details$threads$title,
#  upvotes = thread_details$threads$upvotes
# )
# write_csv(week12_tbl, file = "../data/week12tbl.csv")

week12_tbl <- read_csv(file = "../data/week12tbl.csv", show_col_types = FALSE)
  # pull tbl from saved file
```

```{r}
# Create Corpus
io_corpus_original <- VCorpus(VectorSource(week12_tbl$title))

# Create a function to remove documents with zero text
removeZeroText = function(x) {
    return(nchar(stripWhitespace(x$content)[[1]]) > 0)}

# Preprocessing pipeline (this has a slightly noticable run time)
io_corpus <- io_corpus_original %>%
  tm_map(content_transformer(replace_abbreviation)) %>% # remove abbreviations
  tm_map(content_transformer(replace_contraction)) %>% # replace contractions
  tm_map(content_transformer(str_to_lower)) %>% # make everything lowercase
  tm_map(removeNumbers) %>% # get rid of numbers
  tm_map(removePunctuation) %>% # get rid of puncuation marks
  tm_map(removeWords, stopwords("en")) %>% # enables focus on relevant content
  tm_map(removeWords, c("io psychology", "io psychologist", 
                        "riopsychology", "io psych", "io psychs",
                        "industrial organizational psychologists",
                        "industrial organizational psychology",
                        "organisational psychology", "io", "psychology",
                        "organizational psychology")) %>% # the order matters
  tm_map(stripWhitespace) %>% # get rid of extra white space
  tm_map(content_transformer(lemmatize_words)) %>% # reduces words to base form 
  tm_filter(removeZeroText) # use predefined function to remove docs w/zero text
    ## removeZeroText didn't actually remove anything...

# Function to see how preprocessing is going
compare_them <- function() {
  casenum <- sample(1:951, 1) 
  print(io_corpus_original[[casenum]]$content)
  print(io_corpus[[casenum]]$content)
}
compare_them()
```

```{r}
# create DTM containing unigrams and bigrams
bigram_tokenizer <- function(x) NGramTokenizer(x, Weka_control(min=1, max=2))
io_dtm <- io_corpus %>%
  DocumentTermMatrix(control = list(tokenize = bigram_tokenizer))

# View the DTM
io_dtm_tbl <- io_dtm %>% as.matrix %>% as_tibble
io_dtm_tbl %>%
  View

# Create a DTM with sparse terms eliminated
io_slim_dtm <- removeSparseTerms(io_dtm, .997)

# Check to confirm that I've got the right ratio between 2:1 and 3:1
print(as.integer(
  c(io_dtm$nrow, io_slim_dtm$ncol, io_dtm$nrow/io_slim_dtm$ncol)))
```

lda_gammas

```{r}
# Use latent Dirichlet allocation to categorize posts into topics

# First, remove empty rows
tokenCounts <- apply(io_slim_dtm, 1, sum)
cleaned_io_dtm <- io_slim_dtm[tokenCounts>0,]  # removed 67 docs

# Topic modeling - determine number of topics to extract
# I did not parallelize as it only took a few second to compute
io_dtm_tune <- FindTopicsNumber(
  cleaned_io_dtm,
  topics = seq(2,10,1), # 2 topics to 10 topics jumping by 1 each time
  metrics = c(
    "Griffiths2004",
    "CaoJuan2009",
    "Arun2010",
    "Deveaud2014"),
  verbose = T
)
FindTopicsNumber_plot(io_dtm_tune) # 3-4 topics (intersection location)
  # seeing parabalas on both would be the best case, but I don't see that :(

# Topic modeling - actual modeling
topics_model <- LDA(cleaned_io_dtm, 3) # tested with 3 & 4 topics
  # Selected 3 topics since the gammas & betas are weak either way and there
  # is a lot of overlap in the words associated with each topic

lda_betas <- tidy(topics_model, matrix="beta") # a good beta is 0.7+

lda_betas %>%
  group_by(topic) %>%
  top_n(951, beta) %>%
  arrange(topic, -beta) %>%
  View  
  # Note: new results are created each time this and the above are recalculated

lda_gammas <- tidy(topics_model, matrix="gamma") 

lda_gammas %>%
  group_by(document) %>%
  top_n(1, gamma) %>%
  slice(1) %>%
  ungroup %>%
  mutate(document = as.numeric(document)) %>%
  arrange(document) %>%
  View
  # Note: new results are created each time this and the above are recalculated

# Create a tibble including doc_id, original post title, topic, & probability
topics_tbl <- week12_tbl %>%
  mutate(doc_id = 1:951) %>%
  mutate(doc_id = as.character(doc_id)) %>% # so that I can join it with lda_gammas
  full_join(lda_gammas, by = c("doc_id" = "document")) %>% 
    # merges data and includes probability for all three topics
  mutate(gamma = as.numeric(gamma)) %>% # so that I can order them numerically
  rename(probability = gamma, # rename gamma as probability
         original = title) %>% # rename title as original
  select(doc_id, everything(), -upvotes) %>% # put doc_id first & remove upvotes
  arrange(doc_id, desc(probability)) %>% 
    # arranging probability in descending order so I can then just keep the
    # highest ranked topic for each doc_id
  distinct(doc_id, .keep_all=TRUE) %>% # only keep highest ranked topic per doc_id
  mutate(doc_id = as.numeric(doc_id)) %>% # so that I can reorder by doc_id again
  arrange(doc_id, doc_id) # to help me ensure I'm not missing anything

# Questions
## 1. Using the beta matrix alone, what topics would you conclude your final topic list maps onto? (e.g., topic 1, 2, 3...n each reflect what substantive topic construct? Use your best judgment.)
### Since a good beta value would be around 0.7+ and my highest beta value is 
### only 0.05, this topic mapping is extremely weak. If I have to force three
### topics, they would be 1- career advice, 2- readings & discussions, and 
### 3- research. However, I took a bit of discression in coming up with these.

## 2. Look at the original text of documents with the highest and lowest probabilities assigned to each document. Do your topic names derived from your interpretation of the beta matrix conceptually match with the content of the original posts? What kind of validity evidence does your answer to this question represent?
### Unfortunately, by pulling each of the top and bottom documents associated with 
### each factor and comparing how much they align with the three factors identifed 
### in Question 1, the three factors don't look ideal. The highest and lowest 
### probability documents for Factor 1 both have to do with career & education 
### advice. Factor 2 has the same problem. Then, Factor 3's highest probablity 
### document appears more alligned with Factor 1 and it's lowest probablity 
### document is actually more aligned with research (proposed Factor 3). This 
### confirms what I predicted in Question 1 with such low beta values. The topic ### mapping is extremely weak with a lot of overlap.


## highest probablity for assignment to factor 1 = document 249
print(io_corpus_original[[249]]$content)
## lowest probablity for assignment to factor 1 = document 560
print(io_corpus_original[[560]]$content)
## highest probablity for assignment to factor 2 = document 10
print(io_corpus_original[[10]]$content)
## lowest probablity for assignment to factor 2 = document 486
print(io_corpus_original[[486]]$content)
## highest probablity for assignment to factor 3 = document 140
print(io_corpus_original[[140]]$content)
## highest probablity for assignment to factor 3 = document 7
print(io_corpus_original[[7]]$content)
```

## Visualization

```{r, warning = FALSE}
# Make a pretty word cloud
wordcloud(
  words = names(io_dtm_tbl),
  freq = colSums(io_dtm_tbl),
  colors = brewer.pal(9,"Dark2"),
  max.words=50  # otherwise this becomes a very cluttered wordcloud
)

# This wordcloud shows the top 50 words associated with the r/iopsychology website
# after being processed and assigned to the io_dtm_tbl. The size and colors of the 
# words are associated with the number of times they are mentioned.
```

## Analysis

```{r}
# Create a dataset that contains topics_tbl plus the upvote count
final_tbl <- week12_tbl %>%
  mutate(doc_id = 1:951) %>%
  mutate(doc_id = as.character(doc_id)) %>% # so that I can join it with lda_gammas
  full_join(lda_gammas, by = c("doc_id" = "document")) %>% 
    # merges data and includes probability for all three topics
  mutate(gamma = as.numeric(gamma)) %>% # so that I can order them numerically
  rename(probability = gamma, # rename gamma as probability
         original = title) %>% # rename title as original
  select(doc_id, original, topic, probability, upvotes) %>% 
    # keep upvotes & put at the end
  arrange(doc_id, desc(probability)) %>% 
    # arranging probability in descending order so I can then just keep the
    # highest ranked topic for each doc_id
  distinct(doc_id, .keep_all=TRUE) %>% # only keep highest ranked topic per doc_id
  mutate(doc_id = as.numeric(doc_id)) %>% # so that I can reorder by doc_id again
  arrange(doc_id, doc_id) # to help me ensure I'm not missing anything
```

```{r}
# Run a statistical analysis to determine if upvotes differs by topic
lm_model <- lm(upvotes ~ topic, data = final_tbl)
summary(lm_model)

# Results: The null hypothesis is that there is no relationship between the 
## independent variable and dependent variable. A p-value of 0.6384 means we fail 
## to reject the null hypothesis and conclude that topic does not have a 
## statistically significant effect on upvotes.
## Note: Each time I re-run through the data, I get a different p-value, but it is 
## always well over 0.05
```

```{r}
# Run a machine learning analysis to determine if upvotes differs by topic
holdout_indices <- createDataPartition(final_tbl$upvotes,
                                       p = .25,
                                       list = T)$Resample1
test_tbl <- final_tbl[holdout_indices,] # create test tbl
training_tbl <- final_tbl[-holdout_indices,] # create training tbl for holdout
training_folds <- createFolds(training_tbl$upvotes) # define training folds

# create random forest model using ranger 
lm_model_ml <- train(
  upvotes ~ topic,
  final_tbl,
  method="lm",
  na.action = na.pass,
  preProcess = c("center","scale","zv","nzv","medianImpute"),
  trControl = trainControl(method="cv", 
                           number=10, # number of folds
                           verboseIter=T, 
                           indexOut = training_folds)
)
lm_model_ml
(cv_lm_model_ml <- max(lm_model_ml$results$Rsquared))
(holdout_lm_model_ml <- cor(predict(lm_model_ml, test_tbl, na.action = na.pass), test_tbl$upvotes)^2)

# Results: Again there is no significant relationship between topic and upvotes. 
## Only 1.07% of the variability in upvotes is explained by topic. Furthermore, the ## holdout correlation (essentially zero) indicates that the model predictions have ## very small correlation with the actual upvote values in the holdout dataset.
## Note: Each time I re-run through the data, I get a different R-squared values, 
## but they are always very low.
```
